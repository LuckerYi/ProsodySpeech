import torch

def load_model(hparams):
    from espnet2.tts.fastspeech import FastSpeech

    if hparams.distributed_run:
        model = FastSpeech(
        idim=hparams.n_symbols,
        odim=hparams.num_mels,
        adim=hparams.adim,
        aheads=hparams.aheads,
        elayers=hparams.elayers,
        eunits=hparams.eunits,
        dlayers=hparams.dlayers,
        dunits=hparams.dunits,
        postnet_layers=hparams.postnet_layers,
        postnet_chans=hparams.postnet_chans,
        postnet_filts=hparams.postnet_filts,
        positionwise_layer_type=hparams.positionwise_layer_type,
        positionwise_conv_kernel_size=hparams.positionwise_conv_kernel_size,
        use_scaled_pos_enc=hparams.use_scaled_pos_enc,
        use_batch_norm=hparams.use_batch_norm,
        encoder_normalize_before=hparams.encoder_normalize_before,
        decoder_normalize_before=hparams.decoder_normalize_before,
        is_spk_layer_norm=hparams.is_spk_layer_norm,
        encoder_concat_after=hparams.encoder_concat_after,
        decoder_concat_after=hparams.decoder_concat_after,
        duration_predictor_layers=hparams.duration_predictor_layers,
        duration_predictor_chans=hparams.duration_predictor_chans,
        duration_predictor_kernel_size=hparams.duration_predictor_kernel_size,
        reduction_factor=hparams.reduction_factor,
        spk_embed_dim=hparams.spk_embed_dim,
        spk_embed_integration_type=hparams.spk_embed_integration_type,
        use_gst=hparams.use_gst,
        gst_tokens=hparams.gst_tokens,
        gst_heads=hparams.gst_heads,
        gst_conv_layers=hparams.gst_conv_layers,
        gst_conv_chans_list=hparams.gst_conv_chans_list,
        gst_conv_kernel_size=hparams.gst_conv_kernel_size,
        gst_conv_stride=hparams.gst_conv_stride,
        gst_gru_layers=hparams.gst_gru_layers,
        gst_gru_units=hparams.gst_gru_units,
        # training related
        transformer_enc_dropout_rate=hparams.transformer_enc_dropout_rate,
        transformer_enc_positional_dropout_rate=hparams.transformer_enc_positional_dropout_rate,
        transformer_enc_attn_dropout_rate=hparams.transformer_enc_attn_dropout_rate,
        transformer_dec_dropout_rate=hparams.transformer_dec_dropout_rate,
        transformer_dec_positional_dropout_rate=hparams.transformer_dec_positional_dropout_rate,
        transformer_dec_attn_dropout_rate=hparams.transformer_dec_attn_dropout_rate,
        duration_predictor_dropout_rate=hparams.duration_predictor_dropout_rate,
        postnet_dropout_rate=hparams.postnet_dropout_rate,
        hparams=hparams,
        init_type="xavier_uniform",
        init_enc_alpha=1.0,
        init_dec_alpha=1.0,
        use_masking=False,
        use_weighted_masking=False
        	).cuda()
    else:
        device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
        model = FastSpeech(
        idim=hparams.n_symbols,
        odim=hparams.num_mels,
        adim=hparams.adim,
        aheads=hparams.aheads,
        elayers=hparams.elayers,
        eunits=hparams.eunits,
        dlayers=hparams.dlayers,
        dunits=hparams.dunits,
        postnet_layers=hparams.postnet_layers,
        postnet_chans=hparams.postnet_chans,
        postnet_filts=hparams.postnet_filts,
        positionwise_layer_type=hparams.positionwise_layer_type,
        positionwise_conv_kernel_size=hparams.positionwise_conv_kernel_size,
        use_scaled_pos_enc=hparams.use_scaled_pos_enc,
        use_batch_norm=hparams.use_batch_norm,
        encoder_normalize_before=hparams.encoder_normalize_before,
        decoder_normalize_before=hparams.decoder_normalize_before,
        is_spk_layer_norm=hparams.is_spk_layer_norm,
        encoder_concat_after=hparams.encoder_concat_after,
        decoder_concat_after=hparams.decoder_concat_after,
        duration_predictor_layers=hparams.duration_predictor_layers,
        duration_predictor_chans=hparams.duration_predictor_chans,
        duration_predictor_kernel_size=hparams.duration_predictor_kernel_size,
        reduction_factor=hparams.reduction_factor,
        spk_embed_dim=hparams.spk_embed_dim,
        spk_embed_integration_type=hparams.spk_embed_integration_type,
        use_gst=hparams.use_gst,
        gst_tokens=hparams.gst_tokens,
        gst_heads=hparams.gst_heads,
        gst_conv_layers=hparams.gst_conv_layers,
        gst_conv_chans_list=hparams.gst_conv_chans_list,
        gst_conv_kernel_size=hparams.gst_conv_kernel_size,
        gst_conv_stride=hparams.gst_conv_stride,
        gst_gru_layers=hparams.gst_gru_layers,
        gst_gru_units=hparams.gst_gru_units,
        # training related
        transformer_enc_dropout_rate=hparams.transformer_enc_dropout_rate,
        transformer_enc_positional_dropout_rate=hparams.transformer_enc_positional_dropout_rate,
        transformer_enc_attn_dropout_rate=hparams.transformer_enc_attn_dropout_rate,
        transformer_dec_dropout_rate=hparams.transformer_dec_dropout_rate,
        transformer_dec_positional_dropout_rate=hparams.transformer_dec_positional_dropout_rate,
        transformer_dec_attn_dropout_rate=hparams.transformer_dec_attn_dropout_rate,
        duration_predictor_dropout_rate=hparams.duration_predictor_dropout_rate,
        postnet_dropout_rate=hparams.postnet_dropout_rate,
        hparams=hparams, 
        init_type="xavier_uniform",
        init_enc_alpha=1.0,
        init_dec_alpha=1.0,
        use_masking=False,
        use_weighted_masking=False       	
        	).to(device)

    return model

